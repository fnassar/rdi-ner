# -*- coding: utf-8 -*-

# from google.colab import drive
import os
import pandas as pd
import re
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import torch.nn.functional as F
from camel_tools.ner import NERecognizer


"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MNtEbaoj9CYZBVaibQpz-0sLNVe-ZziL
"""


# !pip install transformers
# !pip install camel_tools
# drive.mount('/content/drive')

# root_dir = "/content/drive/MyDrive/rdi-data/"
# os.makedirs(root_dir, exist_ok=True); os.chdir(root_dir)

"""GIT"""

# !git clone https://$(cat token.txt)@github.com/fnassar/rdi-ner.git
# !apt install tree

# !tree

# os.chdir("/content/drive/MyDrive/rdi-data/rdi-ner")

# !git pull origin main

"""DATA ARRANGING"""


data = open('./task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_train.txt').readlines()
data_test = open('./task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_test.txt').readlines()
data2 = []
data_test2 = []
for line in data:
    data2.append(line.strip('\n').split(' '))

for line in data_test:
    data_test2.append(line.strip('\n').split(' '))


df = pd.DataFrame(data2, columns=['text', 'label'])
df2 = pd.DataFrame(data_test2, columns=['text', 'label'])

df = df[~df['text'].isna()]
df = df[~df['label'].isna()]
df2 = df2[~df2['text'].isna()]
df2 = df2[~df2['label'].isna()]

# def non_arabic_ratio(line):
#     arabic_chars = re.findall('[\u0600-\u06FF]+', line)
#     arabic_len = sum(len(word) for word in arabic_chars)
#     return (len(line) - arabic_len) / len(line)

listt = {'id': [], 'tokens': [], 'sentences': [],
         'ner_tags': [], 'num_Words': [], 'labels': []}
test = pd.DataFrame(listt)
train = pd.DataFrame(listt)
all_data = {
    'test': test,
    'train': train
}

all_labels = []


def add_data(name):

    num_Words = 0
    labels = []
    ner_tags = []
    tokens = []
    sentence = ""
    temp = df
    if (name == 'test'):
        temp = df2
    for column, item in temp.iterrows():
        # Access column name using 'column' and column values using 'values'
        if (item['label'] not in labels):
            labels.append(item['label'])

        sentence += item['text'] + " "
        tokens.append(item['text'])  # words
        ner_tags.append(item['label'])  # tokens
        num_Words += 1
        if (item['label'] not in all_labels):
            all_labels.append(item['label'])

        if (item['text'] == '.' or item['text'] == '،'):

            all_data[name].loc[len(all_data[name].index)] = [len(
                all_data[name].index), tokens, sentence, ner_tags, num_Words, labels]
            num_Words = 0
            labels = []
            ner_tags = []
            tokens = []
            sentence = ""


add_data('train')
add_data('test')

# all_labels

# all_data['train']

"""TRAINING"""


sentence = ".فرانكفورت (د ب أ) أعلن اتحاد صناعة السيارات في ألمانيا امس الاول أن شركات صناعة السيارات في ألمانيا تواجه عاما صعبا في ظل ركود السوق الداخلية والصدرات وهي تسعي لان يبلغ الانتاج حوالي خمسة ملايين سيارة في عام 2002"

model_name = 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner'
model = AutoModelForTokenClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model)

tokens = tokenizer.tokenize(sentence)
ids = tokenizer.convert_tokens_to_ids(tokens)
decode_str = tokenizer.decode(ids)

ner = pipeline('ner', model=model, tokenizer=tokenizer)
res = ner(sentence)


# print(res)
# print(model)
# print(ids)
# print(decode_str)


sentence = ".فرانكفورت (د ب أ) أعلن اتحاد صناعة السيارات في ألمانيا امس الاول أن شركات صناعة السيارات في ألمانيا تواجه عاما صعبا في ظل ركود السوق الداخلية والصدرات وهي تسعي لان يبلغ الانتاج حوالي خمسة ملايين سيارة في عام 2002"

model_name = 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner'
model = AutoModelForTokenClassification.from_pretrained(
    model_name)  # replace with ner odel
tokenizer = AutoTokenizer.from_pretrained(model_name)

ner = pipeline('ner', model=model, tokenizer=tokenizer)

x_train = all_data['train']['sentences'].to_list()

res = ner(sentence)
# print('res')


batch = tokenizer(x_train, padding=True, truncation=True,
                  max_length=1024, return_tensors="pt")  # pt->pytorch format

# print('batch')

with torch.no_grad():
    outputs = model(**batch)  # batch is a dictionary
    print(outputs)
    pred = F.softmax(outputs.logits, dim=1)
    print(pred)
    labels = torch.argmax(pred, dim=1)
    print(labels)


# ner = NERecognizer.pretrained('CAMeL-Lab/bert-base-arabic-camelbert-mix-ner')


# y_pred = []
# y_test = []

# sentence = all_data['test']['sentences'][0].split()
# labels = ner.predict_sentence(sentence)
# y_pred = labels
# y_test = all_data['test']['tokens'][0]
 '''
 
 from google.colab import drive
drive.mount('/content/drive')


root_dir = "/content/drive/MyDrive/rdi-data/"
os.makedirs(root_dir, exist_ok=True); os.chdir(root_dir)

GIT
---

#@title
!git clone https://$(cat token.txt)@github.com/fnassar/rdi-ner.git
!apt install tree
!tree
!git config --global user.email "fan6236@nyu.edu"
!git add .
!git commit -m "data set in prog"
!git pull origin
!git push origin main
os.chdir("/content/drive/MyDrive/rdi-data/rdi-ner")
!git pull origin main
# os.chdir("/content/drive/MyDrive/rdi-data")
# !git

#@title
!tree

#@title
!git config --global user.email "fan6236@nyu.edu"
!git add .
!git commit -m "data set in prog"

#@title
!git pull origin

#@title
!git push origin main

#@title
os.chdir("/content/drive/MyDrive/rdi-data/rdi-ner")
!git pull origin main
# os.chdir("/content/drive/MyDrive/rdi-data")

#@title
# !git

PREPARE DATA
---

!pip install transformers[torch]
!pip install datasets

!ls


data = open('/content/drive/MyDrive/rdi-data/rdi-ner/task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_train.txt').readlines()
data_test = open('/content/drive/MyDrive/rdi-data/rdi-ner/task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_test.txt').readlines()
data2 = []
data_test2 = []
for line in data:
  data2.append(line.strip('\n').split(' '))

for line in data_test:
  data_test2.append(line.strip('\n').split(' '))


df_train = pd.DataFrame(data2, columns=['text', 'label'])
df_test = pd.DataFrame(data_test2, columns=['text', 'label'])

listt={'id':[], 'tokens':[], 'ner_tags':[],'sentences':[]}
#  , 'num_Words':[], 'labelna':[]
test = pd.DataFrame(listt)
train = pd.DataFrame(listt)
dev = pd.DataFrame(listt)
all_data = {
    'test':test,
    'dev':dev,
    'train':train
}

all_labels = []

def add_data(name):

  num_Words = 0
  labels = []
  ner_tags = []
  tokens = []
  sentence = ""
  temp=df_test
  # if(name == 'train'):
  #     print('hi')
  #     temp = df_train
  for column, item in df_train.iterrows() if name == 'train' else df_test.iterrows():

    if (item['text'] == ''):

      all_data[name].loc[len(all_data[name].index)] = [len(all_data[name].index),tokens,ner_tags,sentence]
      #  num_Words labels
      num_Words = 0
      labels =[]
      ner_tags = []
      tokens = []
      sentence = ""

    else:
      # Access column name using 'column' and column values using 'values'
      if (item['label'] not in labels and item['label'] !=None and item['label'] !=''):
        labels.append(item['label'])

      sentence += item['text'] + " "
      tokens.append(item['text']) # words
      ner_tags.append(item['label']) # tokens
      num_Words+=1
      if(item['label'] not in all_labels):
        all_labels.append(item['label'])




add_data('train')
add_data('test')

#split the data into train and test set
train,test = train_test_split(all_data['train'], test_size=0.10, random_state=0)
#save the data
all_data['train'] = train
all_data['dev'] = test


df_train,df_dev = train_test_split(df_train, test_size=0.10, random_state=0)

id2label = {idx:all_labels[idx] for idx in range(len(all_labels))}
id2label

label2id = {v:k for k,v in id2label.items()}
label2id

# all_data['test']
tempp = all_data['test']
tempp

id2label

dtast = datasets.DatasetDict({
    "train": datasets.Dataset.from_pandas(all_data['train']),
    "test": datasets.Dataset.from_pandas(all_data['test']),
    "dev": datasets.Dataset.from_pandas(all_data['dev'])
})
# or torch.util.dataset add init, len, getitem

dtast['dev']

# df_train['label'].value_counts(normalize=True).sort_index()
tempp = all_data['test']
tempp = tempp.explode('ner_tags')
tempp['ner_tags'].value_counts(normalize=True).sort_index()


def convert_ner_tags(tags):
    return [list(id2label.keys())[list(id2label.values()).index(tag)] for tag in tags]

# Apply the function to the ner_tags column
dtast # = dtast.rename_column('ner_tags','ner_tags_text')

# dtast = dtast.map(convert_ner_tags, batched=True)




for split in dtast.keys():
    print(f"{split} column names: {dtast[split].column_names}")


# def convert_ner_tags(tags):
#     return [list(id2label.keys())[list(id2label.values()).index(tag)] for tag in tags]

# # Apply the function to the ner_tags column
# dtast['ner_tags'] = dtast['ner_tags_text'].apply(convert_ner_tags)

# Apply the function to each dataset in the DatasetDict
dtast = dtast.map(convert_ner_tags)

TOKENIZE
---

dtast["train"]['tokens'][2]


model_ckpt = "CAMeL-Lab/bert-base-arabic-camelbert-mix-ner"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

tokenizer(dtast["train"]['tokens'][2])



def tokenize_text(examples):
    return tokenizer(examples['tokens'], truncation=True, max_length=1024)

x_train = all_data['train']['tokens'].to_list()
# x_train

dtast = dtast.map(tokenize_text, batched=True)
dtast

class_weights = (1- (tempp['ner_tags'].value_counts().sort_index()/len(tempp))).values
class_weights

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
class_weigShts = torch.from_numpy(class_weights).float().to(device)
class_weights

dtast

dtast = dtast.rename_column('ner_tags', 'labels')

NEXT TRAIN PREP
---


class WeightedLossTrainer(Trainer):
  def compute_loss(self,model, inputs,return_outputs=False):
    outputs = model(**inputs)
    logits = outputs.get("logits")
    # extract labels
    labels = inputs.get("labels")
    # def loss func with class weights
    loss_func = nn.CrossEntropyLoss(weight = class_weights)
    # compute loss
    loss = loss_func(logits, labels)
    return (loss, outputs) if return_outputs else loss




model = AutoModelForTokenClassification.from_pretrained(model_ckpt,num_labels=9, id2label=id2label, label2id=label2id)




batch_size = 64
# Log the training loss at each epoch
logging_steps = len(dtast['train'])
output_dir = "/content/drive/MyDrive/rdi-data/rdi-ner/task3/model2.py"
training_args = TrainingArguments(output_dir = output_dir,
                                  num_train_epochs =5,
                                  learning_rate = 2e-5,
                                  per_device_train_batch_size = batch_size,
                                  per_device_eval_batch_size = batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy="epoch",
                                  logging_steps = logging_steps
                                  # fp16=True, # mаке it train fast!
                                  # push_to_hub = True
                                  )


trainer = WeightedLossTrainer(
          model=model,
          args=training_args,
          compute_metrics=compute_metrics,
          train_dataset=dtast['train'],
          eval_dataset=dtast['dev'],
          tokenizer=tokenizer
)

dtast['train'][0]

trainer.train()

OTHER DATASET
---


raw_datasets = load_dataset("conll2003")

raw_datasets['train'][0]

dtast['train'][0]
 
 '''