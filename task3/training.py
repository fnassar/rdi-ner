# -*- coding: utf-8 -*-

# from google.colab import drive
import os
import pandas as pd
import re
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import torch.nn.functional as F
from camel_tools.ner import NERecognizer
from sklearn.metrics import f1_score

"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MNtEbaoj9CYZBVaibQpz-0sLNVe-ZziL
"""


# !pip install transformers
# !pip install camel_tools
# drive.mount('/content/drive')

# root_dir = "/content/drive/MyDrive/rdi-data/"
# os.makedirs(root_dir, exist_ok=True); os.chdir(root_dir)

"""GIT"""

# !git clone https://$(cat token.txt)@github.com/fnassar/rdi-ner.git
# !apt install tree

# !tree

# os.chdir("/content/drive/MyDrive/rdi-data/rdi-ner")

# !git pull origin main

"""DATA ARRANGING"""


data = open('./task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_train.txt').readlines()
data_test = open('./task3/ANERcorp-CamelLabSplits/ANERCorp_CamelLab_test.txt').readlines()
data2 = []
data_test2 = []
for line in data:
    data2.append(line.strip('\n').split(' '))

for line in data_test:
    data_test2.append(line.strip('\n').split(' '))


df = pd.DataFrame(data2, columns=['text', 'label'])
df2 = pd.DataFrame(data_test2, columns=['text', 'label'])

df = df[~df['text'].isna()]
df = df[~df['label'].isna()]
df2 = df2[~df2['text'].isna()]
df2 = df2[~df2['label'].isna()]

# def non_arabic_ratio(line):
#     arabic_chars = re.findall('[\u0600-\u06FF]+', line)
#     arabic_len = sum(len(word) for word in arabic_chars)
#     return (len(line) - arabic_len) / len(line)

listt = {'id': [], 'tokens': [], 'sentences': [],
         'ner_tags': [], 'num_Words': [], 'labels': []}
test = pd.DataFrame(listt)
train = pd.DataFrame(listt)
all_data = {
    'test': test,
    'train': train
}

all_labels = []


def add_data(name):

    num_Words = 0
    labels = []
    ner_tags = []
    tokens = []
    sentence = ""
    temp = df
    if (name == 'test'):
        temp = df2
    for column, item in temp.iterrows():
        # Access column name using 'column' and column values using 'values'
        if (item['label'] not in labels):
            labels.append(item['label'])

        sentence += item['text'] + " "
        tokens.append(item['text'])  # words
        ner_tags.append(item['label'])  # tokens
        num_Words += 1
        if (item['label'] not in all_labels):
            all_labels.append(item['label'])

        if (item['text'] == '.' or item['text'] == '،'):

            all_data[name].loc[len(all_data[name].index)] = [len(
                all_data[name].index), tokens, sentence, ner_tags, num_Words, labels]
            num_Words = 0
            labels = []
            ner_tags = []
            tokens = []
            sentence = ""


add_data('train')
add_data('test')

# all_labels

# all_data['train']

"""TRAINING"""


sentence = ".فرانكفورت (د ب أ) أعلن اتحاد صناعة السيارات في ألمانيا امس الاول أن شركات صناعة السيارات في ألمانيا تواجه عاما صعبا في ظل ركود السوق الداخلية والصدرات وهي تسعي لان يبلغ الانتاج حوالي خمسة ملايين سيارة في عام 2002"

model_name = 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner'
model = AutoModelForTokenClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)


tokens = tokenizer.tokenize(sentence)
ids = tokenizer.convert_tokens_to_ids(tokens)
decode_str = tokenizer.decode(ids)

ner = pipeline('ner', model=model, tokenizer=tokenizer)
res = ner(sentence)


# print(res)
# print(model)
# print(ids)
# print(decode_str)


sentence = ".فرانكفورت (د ب أ) أعلن اتحاد صناعة السيارات في ألمانيا امس الاول أن شركات صناعة السيارات في ألمانيا تواجه عاما صعبا في ظل ركود السوق الداخلية والصدرات وهي تسعي لان يبلغ الانتاج حوالي خمسة ملايين سيارة في عام 2002"

model_name = 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner'
model = AutoModelForTokenClassification.from_pretrained(
    model_name)  # replace with ner odel
tokenizer = AutoTokenizer.from_pretrained(model_name)

ner = pipeline('ner', model=model, tokenizer=tokenizer)

x_train = all_data['train']['sentences'].to_list()

res = ner(sentence)
# print('res')


batch = tokenizer(x_train, padding=True, truncation=True,
                  max_length=1024, return_tensors="pt")  # pt->pytorch format

# print('batch')

with torch.no_grad():
    outputs = model(**batch)  # batch is a dictionary
    print(outputs)
    pred = F.softmax(outputs.logits, dim=1)
    print(pred)
    labels = torch.argmax(pred, dim=1)
    print(labels)


# ner = NERecognizer.pretrained('CAMeL-Lab/bert-base-arabic-camelbert-mix-ner')


# y_pred = []
# y_test = []

# sentence = all_data['test']['sentences'][0].split()
# labels = ner.predict_sentence(sentence)
# y_pred = labels
# y_test = all_data['test']['tokens'][0]
